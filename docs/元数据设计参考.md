# 元数据采集器设计文档

> 版本：1.0

## 一、目的与范围

本设计文档描述一个可扩展的元数据采集器（Metadata Collector）。目标是统一采集以下类型数据源的结构化与非结构化元数据，并提供标准化接口与返回数据格式，便于新增采集器只需继承/实现接口即可接入：

1. 传统关系型数据库（MySQL、Oracle、PostgreSQL、SQL Server）
2. 大数据数仓（Hive、Doris、ClickHouse、Greenplum 等）
3. 大数据数据湖（基于对象存储的 Parquet/ORC/Delta Lake 等）
4. 非结构化采集：文件系统、OSS、MinIO、HDFS 上的非结构化文件

适用场景：元数据管理、血缘分析、数据质量、权限审计、数据目录。


## 三、设计原则

- **统一接口**：所有 Connector 实现同一套接口，返回标准化元数据模型。
- **可插拔**：新增采集器只需实现接口并注册到框架。
- **幂等与增量**：支持全量与增量采集，结果保证幂等写入。
- **容错与重试**：Connector 层实现退避重试策略；Core 做限流、并发控制。
- **可观测**：采集任务暴露指标（成功率、耗时、错误率、采集量）并记录日志与采集历史。
- **可扩展**：支持水平扩展和分布式调度。


## 四、核心接口定义（伪代码 / API）

> 说明：接口采用语言无关的定义（兼容 REST/gRPC/Java/Python/Go 接口实现）。示例使用 JSON-RPC 风格描述接口与返回模型。

### 4.1 Connector 基础接口（Collector Interface）

```
interface IMetadataConnector {
    // 初始化：传入配置、凭证
    init(config: ConnectorConfig) -> void

    // 健康检查
    healthCheck() -> HealthStatus

    // 探测（发现）数据源下的 catalog / schema / databases
    discoverCatalogs() -> List<CatalogInfo>

    // 列出 schema 下的表/对象（支持分页）
    listTables(catalog: string, schema: string, pageToken?: string, pageSize?: int) -> TableListResult

    // 获取单个表的结构化元数据（列、类型、注释、主键、索引、partition）
    fetchTableMetadata(catalog: string, schema: string, table: string) -> TableMetadata

    // 获取表统计信息（行数、数据大小、列基数、null 比例）
    fetchTableStatistics(catalog: string, schema: string, table: string) -> TableStatistics

    // 获取表数据样例（可选）
    fetchSampleData(catalog: string, schema: string, table: string, limit: int) -> SampleData

    // 获取分区信息（若有）
    fetchPartitions(catalog: string, schema: string, table: string) -> PartitionInfo[]

    // 非结构化资源：列出路径下文件元信息
    listFiles(path: string, pageToken?: string) -> FileListResult

    // 触发全量/增量采集任务（可选）
    startCollection(mode: CollectionMode, options: Map) -> TaskHandle

    // 关闭/卸载 connector
    close() -> void
}
```

**ConnectorConfig**（示例字段）

```
{
  "id": "connector-mysql-01",
  "type": "mysql",
  "endpoint": "jdbc:mysql://192.168.0.20:3306",
  "credentials": {"user":"xxx","password":"yyy"},
  "properties": {
    "driver": "com.mysql.cj.jdbc.Driver",
    "useSSL": false,
    "connectionTimeout": 30,
    "sample": {
      "rows": 100               /* 抽样行数*/
    },
    "matching": {
      "patternType": "glob",          /* "glob" 或 "regex" */
      "caseSensitive": false,
      "databases": {                  /* 针对库/模式 级别的白黑名单与映射 */
        "include": ["db_*", "sales"], /* glob 或 regex 由 patternType 决定 */
        "exclude": ["db_test", "tmp_*"],
      
      },
      "schemas": {                     /* 有些系统存在 schema 与 database 的区分（如 Postgres） */
        "include": ["**"],
        "exclude": []
      },
      "tables": {                      /* 表级白/黑名单、列出/采样策略 */
        "include": ["orders", "user_*"],
        "exclude": ["tmp_%", "backup_*"]      
      }     
    },

    /* 额外：是否采集表结构以外的元数据，如分区、索引、注释等 */
    "collect": {
      "partitions": true,
      "indexes": true,
      "comments": true,
      "statistics": false
    },
    "statisticsConfig": {
      "enabled": true,              // 是否开启统计
      "level": "table",             // table / column / full
      "tableStats": {
        "rowCount": true,           // 表行数
        "dataSize": true,           // 表存储大小
        "partitionCount": true      // 分区数量
      },
      "columnStats": {
        "nullCount": true,          // null 数量
        "distinctCount": true,      // 去重值个数
        "min": true,
        "max": true,
        "avg": false,
        "median": false,
        "topN": 5                   // 频次最高排名前 N 个值
      },    
      "performance": {
        "maxTimeSeconds": 30,       // 最大统计时间
        "maxRows": 5000000          // 避免统计全表
      }
    }

  }
}

```

## 五、返回数据模型（标准化 JSON schema）

> 为了统一不同数据源的返回，定义一套通用的元数据模型。各 Connector 应将本地来源映射到以下模型。

### 5.1 CatalogInfo

```json
{
  "catalog": "string",
  "type": "mysql|hive|doris|s3|filesystem",
  "description": "string",
  "properties": { }
}
```


### 5.2 TableMetadata

```json
{
  "catalog": "string",
  "schema": "string",
  "name": "string",
  "type": "TABLE | VIEW | EXTERNAL_TABLE | FILE",
  "comment": "string",

  "columns": [
      {
      "ordinalPosition": 1,
      "name": "string",

      "type": "string",                       // 标准化类型（系统内部统一后的类型）
      "sourceType": "varchar(255)",           // ★ 原始数据库类型（未解析）
      "length": 255,                           // 字符串长度
      "precision": 10,                         // 精度
      "scale": 2,                              // scale

      "nullable": true,
      "default": "string",
      "comment": "string",

      "isPrimaryKey": false,
      "isPartitionColumn": false,
      "isAutoIncrement": false,

      "raw": {                                 // ★ 完整保留数据库返回的原始元数据
        /* 建议原封不动存下 DatabaseMetaData / SHOW COLUMNS / DESCRIBE 的 Raw 字段 */
        }
      }

  ],

  "partitions": [
    /* PartitionInfo */
  ],

  "stats": {
    /* TableStatistics */
  },

  "storage": {
    "format": "parquet | orc | delta | csv | json | avro",
    "location": "s3://bucket/path or hdfs://...",
    "inputFormat": "optional",
    "serde": "optional",
    "compressed": true                    // ★ 是否压缩，许多数据湖需要
  },

  "properties": {
    /* 原始源特有属性，如 MySQL ENGINE，Hive TBLPROPERTIES */
  },

  "lastRefreshedAt": "2025-12-10T12:00:00Z"
}

```


### 5.3 TableStatistics

```json
{
  "rowCount": 123456,                       // 表总行数
  "dataSizeBytes": 123456789,               // 表总大小
  "partitionCount": 12,  
                     // 分区数（如果有分区表）
  "columnStats": [
    {
      "name": "col1",
      "distinctCount": 1000,
      "nullCount": 10,
      "min": "a@example.com",
      "max": "z@example.com",
      "avg": null,
      "stddev": null,
      "topN": [
        {"value": "a@example.com", "count": 100},
        {"value": "b@example.com", "count": 80}
      ],
      "percentiles": {                        // 可选
        "p25": "a@example.com",
        "p50": "m@example.com",
        "p75": "x@example.com"
      }
    },
    {
      "name": "amount",
      "type": "decimal",
      "sourceType": "decimal(10,2)",
      "logicalType": "amount",
      "distinctCount": 500,
      "nullCount": 0,
      "min": 0,
      "max": 10000,
      "avg": 1234.56,
      "stddev": 234.56,
      "topN": [
        {"value": 100, "count": 200},
        {"value": 200, "count": 150}
      ],
      "percentiles": {
        "p25": 50,
        "p50": 1200,
        "p75": 3000
      }
    }
  ]
}

```


### 5.4 File Metadata（用于数据湖 / OSS / MinIO / 文件系统）

```json
{
  "path": "s3://bucket/path/2025/12/",
  "name": "part-0000.parquet",
  "size": 1234567,
  "format": "parquet|csv|json",
  "lastModified": "2025-12-10T10:00:00Z",
  "checksum": "md5/sha256",
  "schema": { /* optional: inferred schema */ },
  "tags": { }
}
```


### 5.5 Search / Registry 索引文档（用于 ES 或关系型存储）

```json
{
  "resourceId":"...",
  "resourceType":"table|file|view",
  "displayName":"schema.table",
  "description":"...",
  "columns": [{"name":"","type":"","comment":""}],
  "owner":"team_x",
  "tags":["pii","finance"],
  "createdAt":"...",
  "updatedAt":"..."
}
```


## 六、各类数据源特别说明（映射与注意点）

### 6.1 传统关系型数据库（MySQL/Oracle/Postgres/SQLServer）

- 通过 JDBC 或原生协议连接，读取 INFORMATION_SCHEMA 或 Catalog 接口。
- 采集项：database、schema、table、view、column、pk、fk、index、trigger、table comment、column comment、表统计（估算行数）
- 支持读取慢查询/DDL 审计日志（如 binlog/redo）用于近实时变更捕获（CDC）。

**返回示例**：见 TableMetadata


### 6.2 大数据数仓（Hive/Doris/ClickHouse）

- Hive/Metastore 或 Catalog API 获取表与分区信息，支持读取表属性（如 serde、location）。
- 对于 Doris/ClickHouse 等，使用对应的 catalog/HTTP API 获取表结构与统计信息。
- 重点采集 partition、分区字段、分区类型、分区范围和分区数量。


### 6.3 数据湖（Parquet/ORC/Delta）：

- 通过对象存储（S3/MinIO/OSS）列出目录并识别文件格式，必要时读取文件头（schema）进行推断。
- 对于分区式数据，解析路径约定（如 /dt=2025-12-10/）以生成逻辑表视图。
- 支持 Delta Lake / Iceberg 需要访问元表（_delta_log / metadata table）以获取事务与版本信息。


### 6.4 非结构化（文件系统 / OSS / MinIO / HDFS）

- 列出目录并读取文件元信息（size、mtime、checksum）。
- 对文本/JSON/CSV 等，尝试采样并推断 schema（可选，开关控制）。
- 对二进制或大文件，仅保留元信息和 MIME 类型。


## 七、采集策略与模式

- **全量采集**：首次采集或显式触发，遍历所有对象并写入目标存储（建议启用并行扫描）。
- **增量采集**：基于最后更新时间（mtime）或数据源的变更日志（binlog、metastore events、S3 event）进行增量扫描。
- **事件驱动**：监听消息（S3 event、CDC、Metastore events）触发即时采集。
- **混合模式**：定期全量（每日/周）+ 实时增量事件。




## 九、错误处理 & 重试策略

- Connector 层需要抛出明确错误码（如 AUTH_ERROR、NETWORK_ERROR、TIMEOUT、UNSUPPORTED_FEATURE）。
- Core 层实现重试政策（指数退避）和幂等写入以避免重复数据。
- 对于部分失败（例如某些分区读取失败），记录失败详情并继续处理其他对象。






## 十三、示例：MySQL Connector 返回示例

```json
{
  "catalog":"mysql_prod",
  "schema":"sales",
  "name":"orders",
  "type":"TABLE",
  "columns":[
    {"name":"id","type":"bigint","nullable":false,"isPrimaryKey":true,"comment":"主键"},
    {"name":"user_id","type":"int","nullable":false},
    {"name":"amount","type":"decimal(10,2)","nullable":false}
  ],
  "stats": {"rowCount": 1200000, "dataSizeBytes": 1024*1024*50},
  "lastRefreshedAt": "2025-12-10T12:00:00Z"
}
```


## 十四、示例：S3/MinIO 上 Parquet 表的返回示例

```json
{
  "catalog":"datalake",
  "schema":"events",
  "name":"page_views",
  "type":"EXTERNAL_TABLE",
  "storage": {"format":"parquet","location":"s3://prod/events/page_views/"},
  "columns":[{"name":"ts","type":"timestamp"},{"name":"user_id","type":"string"}],
  "partitions":[{"name":"dt","type":"date","valuesCount":365}],
  "stats":{"rowCount": 100000000}
}
```



## 十六、扩展场景

- **血缘分析**：设计时预留字段 capture SQL DDL/ETL 脚本、视图依赖，用以构建上游/下游依赖图。
- **数据质量**：采集统计可与质量规则引擎联动。


